{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvTFr6t7JBUF"
      },
      "source": [
        "# Generative Adversarial Network (GAN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyyxWnT0IYNz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as trans\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKLTBojGTjHE"
      },
      "source": [
        "# Data directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgbgtHLgTjYD"
      },
      "outputs": [],
      "source": [
        "data_dir = 'dataset/'\n",
        "print(os.listdir(data_dir)[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RujKRnuU7xz"
      },
      "source": [
        "# Defining training data and loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTDKxma1U7-R"
      },
      "outputs": [],
      "source": [
        "image_size = 64\n",
        "batch_size = 128\n",
        "mean = (0.5, 0.5, 0.5)\n",
        "std = (0.5, 0.5, 0.5)\n",
        "train_data = ImageFolder(\n",
        "    data_dir,\n",
        "    transform = trans.Compose(\n",
        "        [\n",
        "         trans.Resize(image_size),\n",
        "         trans.RandomRotation(5),\n",
        "         trans.RandomHorizontalFlip(0.5),\n",
        "         trans.CenterCrop(image_size),\n",
        "         trans.ToTensor(),\n",
        "         trans.Normalize(mean = mean, std = std)\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size,\n",
        "    shuffle = True,\n",
        "    num_workers = 2,\n",
        "    pin_memory = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffebidY2rg7u"
      },
      "source": [
        "# Display images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpeuKYdirhIY"
      },
      "outputs": [],
      "source": [
        "def denorm(img_tensors):\n",
        "    return img_tensors * std[0] + mean[0]\n",
        "\n",
        "def show_images(images, nmax=8):\n",
        "    fig, plot = plt.subplots(figsize=(8, 8))\n",
        "    plt.title(\"Cats images for GAN\")\n",
        "    plt.axis(\"off\")\n",
        "    plot.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
        "\n",
        "def show_batch(dl, nmax=64):\n",
        "    for images, _ in dl:\n",
        "        show_images(images, nmax)\n",
        "        break\n",
        "\n",
        "show_batch(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsqm6KlxGSQm"
      },
      "source": [
        "# Select device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjmr7uwfGSaG",
        "outputId": "fed940ef-81a9-45fd-93d5-186f4ac6a088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ7jBnVNHbtX"
      },
      "source": [
        "# Discriminator description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_JXrv-_Hb4y"
      },
      "outputs": [],
      "source": [
        "discriminator = nn.Sequential(\n",
        "    \n",
        "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "\n",
        "    nn.Flatten(),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "discriminator = discriminator.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkFL_e7KIdN5"
      },
      "source": [
        "# Generator description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92MRCbtuIdXD"
      },
      "outputs": [],
      "source": [
        "generator = nn.Sequential(\n",
        "\n",
        "    nn.ConvTranspose2d(128, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.ReLU(True),\n",
        "\n",
        "    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.ReLU(True),\n",
        "\n",
        "    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ReLU(True),\n",
        "\n",
        "    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(True),\n",
        "\n",
        "    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.Tanh()\n",
        ")\n",
        "\n",
        "generator = generator.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KChOrspHJymN"
      },
      "source": [
        "# Train discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Imun1kzPJyvk"
      },
      "outputs": [],
      "source": [
        "def train_disc(real_images, disc):\n",
        "  disc.zero_grad()\n",
        "\n",
        "  real_prediction = discriminator(real_images)\n",
        "  real_target = torch.ones(real_images.size(0), 1, device = device)\n",
        "  real_loss = F.binary_cross_entropy(real_prediction, real_target)\n",
        "  real_score = torch.mean(real_prediction).item()\n",
        "\n",
        "  latent = torch.randn(batch_size, 128, 1, 1, device=device)\n",
        "  fake_images = generator(latent)\n",
        "\n",
        "  fake_target = torch.zeros(fake_images.size(0), 1, device = device)\n",
        "  fake_prediction = discriminator(fake_images)\n",
        "  fake_loss = F.binary_cross_entropy(fake_prediction, fake_target)\n",
        "  fake_score = torch.mean(fake_prediction).item()\n",
        "\n",
        "  loss = real_loss + fake_loss\n",
        "  loss.backward()\n",
        "  disc.step()\n",
        "  return loss.item(), real_score, fake_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gophipdmNQfT"
      },
      "source": [
        "# Train generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhVfXpcXNQsZ"
      },
      "outputs": [],
      "source": [
        "def train_gen(gen):\n",
        "  gen.zero_grad()\n",
        "\n",
        "  latent = torch.randn(batch_size, 128, 1, 1, device=device)\n",
        "  fake_images = generator(latent)\n",
        "\n",
        "  prediction = discriminator(fake_images)\n",
        "  target = torch.ones(batch_size, 1, device = device)\n",
        "  loss = F.binary_cross_entropy(prediction, target)\n",
        "\n",
        "  loss.backward()\n",
        "  gen.step()\n",
        "\n",
        "  return loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUvnfoMeAA7A"
      },
      "source": [
        "# Save generated images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa30dPhkABEU"
      },
      "outputs": [],
      "source": [
        "newfolder = 'generated_images'\n",
        "os.makedirs(newfolder, exist_ok=True)\n",
        "\n",
        "def save_generated_images(index, latent_tensors):\n",
        "  fake_images = generator(latent_tensors)\n",
        "  fake_file = 'output-{0:0=4d}.png'.format(index)\n",
        "  save_image(denorm(fake_images), os.path.join(newfolder, fake_file), nrow = 8)\n",
        "\n",
        "fixed_latent = torch.randn(64, 128, 1, 1, device=device)\n",
        "save_generated_images(0, fixed_latent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go30DnQWBB3k"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JAEliXcBCAe"
      },
      "outputs": [],
      "source": [
        "def model (epochs, learning_rate, start_index = 1):\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  real_scores = []\n",
        "  fake_scores = []\n",
        "  loss_gen = []\n",
        "  loss_disc = []\n",
        "  print(discriminator.parameters())\n",
        "  print(generator.parameters())\n",
        "  genopt = torch.optim.Adam(generator.parameters(), lr = learning_rate, betas = (0.5, 0.999))\n",
        "  discopt = torch.optim.Adam(discriminator.parameters(), lr = learning_rate, betas = (0.5, 0.999))\n",
        "  for epoch in range(epochs):\n",
        "    for real_images, _ in tqdm(train_loader):\n",
        "      real_images = real_images.to(device)\n",
        "      discloss, real_score, fake_score = train_disc(real_images, discopt)\n",
        "      genloss = train_gen(genopt)\n",
        "    loss_gen.append(genloss)\n",
        "    loss_disc.append(discloss)\n",
        "    real_scores.append(real_score)\n",
        "    fake_scores.append(fake_score)\n",
        "\n",
        "    print(\"Epoch [{}/{}], genloss: {:.4f}, discloss: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
        "        epoch+1, epochs, genloss, discloss, real_score, fake_score))\n",
        "    \n",
        "    save_generated_images(epoch+start_index, fixed_latent)\n",
        "    \n",
        "  return loss_gen, loss_disc, real_scores, fake_scores\n",
        "\n",
        "learning_rate = 0.0002\n",
        "epochs = 50\n",
        "\n",
        "history = model(epochs, learning_rate)\n",
        "\n",
        "loss_gen, loss_disc, real_scores, fake_scores = history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR4Z39qFDRmZ"
      },
      "source": [
        "# Scores graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZQF9O-2DRwE"
      },
      "outputs": [],
      "source": [
        "plt.plot(real_scores, '-')\n",
        "plt.plot(fake_scores, '-')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('score')\n",
        "plt.legend(['Real', 'Fake'])\n",
        "plt.title('Real and fake scores');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ0164brDz3E"
      },
      "source": [
        "# Losses graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc0kWB-uD0hg"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_disc, '-')\n",
        "plt.plot(loss_gen, '-')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Discriminator', 'Generator'])\n",
        "plt.title('Generator and Discriminator loss during training');"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CatGAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}